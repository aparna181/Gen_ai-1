{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aparna181/Gen_ai-1/blob/main/gen_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKPIWrWca_xh"
      },
      "source": [
        "#Data extraction project from png files using MultiModal LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q2jq_E-xaziL"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q -U google-generativeai    #Installing google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QlHMpY6_c8Y4"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai # Importing the library that was installed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's configure the gemini key\n",
        "# setup api key\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('gemini_key')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "mBQYtfYrq0mV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list of models from google genai\n",
        "for model in genai.list_models():\n",
        "    # if 'generateContent' in model.supported_generation_methods:\n",
        "    #   print(model.name)\n",
        "    print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ca2YqOQcq-ou",
        "outputId": "56ec44c4-f788-4ee4-a7bc-9fca2ac2540d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/chat-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='PaLM 2 Chat (Legacy)',\n",
            "      description='A legacy text-only model optimized for chat conversations',\n",
            "      input_token_limit=4096,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
            "      temperature=0.25,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/text-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='PaLM 2 (Legacy)',\n",
            "      description='A legacy model that understands text and generates text as an output',\n",
            "      input_token_limit=8196,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
            "      temperature=0.7,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/embedding-gecko-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding Gecko',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=1024,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Latest',\n",
            "      description=('The original Gemini 1.0 Pro model. This model will be discontinued on '\n",
            "                   'February 15th, 2025. Move to a newer Gemini version.'),\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro',\n",
            "      description='The best model for scaling across a wide range of tasks',\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro',\n",
            "      description='The best model for scaling across a wide range of tasks',\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
            "      description=('The original Gemini 1.0 Pro model version that supports tuning. Gemini 1.0 '\n",
            "                   'Pro will be discontinued on February 15th, 2025. Move to a newer Gemini '\n",
            "                   'version.'),\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-vision-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
            "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
            "                   'Move to a newer Gemini version.'),\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/gemini-pro-vision',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
            "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
            "                   'Move to a newer Gemini version.'),\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/gemini-1.5-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
            "                   'million tokens.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-pro-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro 001',\n",
            "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
            "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-pro-002',\n",
            "      base_model_id='',\n",
            "      version='002',\n",
            "      display_name='Gemini 1.5 Pro 002',\n",
            "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
            "                   'supports up to 2 million tokens, released in September of 2024.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro',\n",
            "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
            "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-pro-exp-0801',\n",
            "      base_model_id='',\n",
            "      version='exp-0801',\n",
            "      display_name='Gemini 1.5 Pro Experimental 0801',\n",
            "      description=('Experimental release (August 1st, 2024) of Gemini 1.5 Pro, our mid-size '\n",
            "                   'multimodal model that supports up to 2 million tokens, with across the board '\n",
            "                   'improvements. Replaced by Gemini-1.5-pro-002 (stable).'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-pro-exp-0827',\n",
            "      base_model_id='',\n",
            "      version='exp-0827',\n",
            "      display_name='Gemini 1.5 Pro Experimental 0827',\n",
            "      description=('Experimental release (August 27th, 2024) of Gemini 1.5 Pro, our mid-size '\n",
            "                   'multimodal model that supports up to 2 million tokens, with across the board '\n",
            "                   'improvements. Replaced by Gemini-1.5-pro-002 (stable).'),\n",
            "      input_token_limit=2000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
            "                   'across diverse tasks.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 001',\n",
            "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
            "                   'for scaling across diverse tasks, released in May of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-001-tuning',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 001 Tuning',\n",
            "      description=('Version of Gemini 1.5 Flash that supports tuning, our fast and versatile '\n",
            "                   'multimodal model for scaling across diverse tasks, released in May of 2024.'),\n",
            "      input_token_limit=16384,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash',\n",
            "      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
            "                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-exp-0827',\n",
            "      base_model_id='',\n",
            "      version='exp-0827',\n",
            "      display_name='Gemini 1.5 Flash Experimental 0827',\n",
            "      description=('Experimental release (August 27th, 2024) of Gemini 1.5 Flash, our fast and '\n",
            "                   'versatile multimodal model for scaling across diverse tasks, with across the '\n",
            "                   'board improvements. Replaced by Gemini-1.5-flash-002 (stable).'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-002',\n",
            "      base_model_id='',\n",
            "      version='002',\n",
            "      display_name='Gemini 1.5 Flash 002',\n",
            "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
            "                   'for scaling across diverse tasks, released in September of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B',\n",
            "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
            "                   'Flash model, released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B 001',\n",
            "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
            "                   'Flash model, released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash-8B Latest',\n",
            "      description=('Alias that points to the most recent production (non-experimental) release '\n",
            "                   'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
            "                   'released in October of 2024.'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
            "      description=('Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our '\n",
            "                   'smallest and most cost effective Flash model. Replaced by '\n",
            "                   'Gemini-1.5-flash-8b-001 (stable).'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-1.5-flash-8b-exp-0924',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 8B Experimental 0924',\n",
            "      description=('Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our '\n",
            "                   'smallest and most cost effective Flash model. Replaced by '\n",
            "                   'Gemini-1.5-flash-8b-001 (stable).'),\n",
            "      input_token_limit=1000000,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/learnlm-1.5-pro-experimental',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='LearnLM 1.5 Pro Experimental',\n",
            "      description=('Alias that points to the most recent stable version of Gemini 1.5 Pro, our '\n",
            "                   'mid-size multimodal model that supports up to 2 million tokens.'),\n",
            "      input_token_limit=32767,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-exp-1114',\n",
            "      base_model_id='',\n",
            "      version='exp-1114',\n",
            "      display_name='Gemini Experimental 1114',\n",
            "      description='Experimental release (November 11th, 2024) of Gemini.',\n",
            "      input_token_limit=32767,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/embedding-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding 001',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/text-embedding-004',\n",
            "      base_model_id='',\n",
            "      version='004',\n",
            "      display_name='Text Embedding 004',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/aqa',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Model that performs Attributed Question Answering.',\n",
            "      description=('Model trained to return answers to questions that are grounded in provided '\n",
            "                   'sources, along with estimating answerable probability.'),\n",
            "      input_token_limit=7168,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateAnswer'],\n",
            "      temperature=0.2,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Configuration\n",
        "MODEL_CONFIG = {\n",
        "  \"temperature\": 0.2, # 0 -1 # we set creativness of the model in generating a response\n",
        "  \"top_p\": 1, # ensures the cumulatively probability is 1\n",
        "  \"top_k\": 32, # will consider top 32 tokens with highest probability\n",
        "  \"max_output_tokens\": 4096, # maximum output tokens generated by the model\n",
        "}\n",
        "\n",
        "## Safety Settings of Model\n",
        "safety_settings = [\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  }\n",
        "]"
      ],
      "metadata": {
        "id": "RD0gAzAQrRUm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD GEMINI MODEL WITH MODEL CONFIGURATIONS\n",
        "model = genai.GenerativeModel(model_name = \"gemini-1.5-flash\",\n",
        "                              generation_config = MODEL_CONFIG,\n",
        "                              safety_settings = safety_settings)"
      ],
      "metadata": {
        "id": "HLSLczoXrWZj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DEFINE IMAGE FORMAT TO INPUT IN GEMINI\n",
        "from pathlib import Path\n",
        "\n",
        "def image_format(image_path): # /content/invoice.png\n",
        "    img = Path(image_path)\n",
        "\n",
        "    if not img.exists():\n",
        "        raise FileNotFoundError(f\"Could not find image: {img}\")\n",
        "\n",
        "    image_parts = [\n",
        "        {\n",
        "            \"mime_type\": \"image/png\", ## Mime type are PNG - image/png. JPEG - image/jpeg. WEBP - image/webp\n",
        "            \"data\": img.read_bytes()\n",
        "        }\n",
        "    ]\n",
        "    return image_parts"
      ],
      "metadata": {
        "id": "JeRpnraDrbqP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gemini pro model output\n",
        "def gemini_output(image_path, system_prompt, user_prompt):\n",
        "\n",
        "    image_info = image_format(image_path) # [number] # number -image path is being sent to image_format function we created above that will give me output in terms of bytes\n",
        "    input_prompt= [system_prompt, image_info[0], user_prompt]\n",
        "    response = model.generate_content(input_prompt)\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "X6hX9MQgrkEs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "               You are a specialist in comprehending receipts.\n",
        "               Input images in the form of receipts will be provided to you,\n",
        "               and your task is to respond to questions based on the content of the input image.\n",
        "               \"\"\"\n",
        "\n",
        "image_path = \"/content/invoice.png\"\n",
        "\n",
        "user_prompt = \"What is the invoice date?\"\n",
        "\n",
        "gemini_output(image_path, system_prompt, user_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QaDWG86tr1Gp",
        "outputId": "43bf2a43-3b27-4895-ec5f-744191f8aa5b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The invoice date is 12/08/2011.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "               You are a specialist in comprehending receipts.\n",
        "               Input images in the form of receipts will be provided to you,\n",
        "               and your task is to respond to questions based on the content of the input image.\n",
        "               \"\"\"\n",
        "\n",
        "image_path = \"/content/invoice.png\"\n",
        "\n",
        "user_prompt = \"What is the total amount?\"\n",
        "\n",
        "gemini_output(image_path, system_prompt, user_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lwHyIdEzr7IS",
        "outputId": "32409331-8b0c-46bf-a0cb-432fca6e4e79"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The total amount is 901.80 EUR.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "               You are a specialist in comprehending receipts.\n",
        "               Input images in the form of receipts will be provided to you,\n",
        "               and your task is to respond to questions based on the content of the input image.\n",
        "               \"\"\"\n",
        "#system_prompt = \"Convert Invoice data into json format with appropriate json tags as required for the data in image \"\n",
        "image_path =  \"/content/handwritten.png\"\n",
        "user_prompt = \"Convert Invoice data into json format with appropriate json tags as required for the data in image \"\n",
        "output = gemini_output(image_path, system_prompt, user_prompt)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CtjvTGVdsDF1",
        "outputId": "af5befdb-53e4-4d30-f20a-d91ed13c1998"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"supplier\": {\n",
            "    \"name\": \"RedmineCRM\",\n",
            "    \"address\": \"Company representative name\\nYour company address\",\n",
            "    \"tax_id\": null, \n",
            "    \"phone\": null,\n",
            "    \"fax\": null\n",
            "  },\n",
            "  \"client\": {\n",
            "    \"name\": \"\\\"Romashka\\\" Ltd.\",\n",
            "    \"address\": \"1600 Amphitheatre Parkway Mountain View, CA 94043\"\n",
            "  },\n",
            "  \"invoice_number\": \"INV/20111209-22\",\n",
            "  \"invoice_date\": \"12/08/2011\",\n",
            "  \"due_date\": \"12/25/2012\",\n",
            "  \"items\": [\n",
            "    {\n",
            "      \"item_number\": 1,\n",
            "      \"description\": \"Projecting\\n- Context menu for invoices list\",\n",
            "      \"quantity\": 1.0,\n",
            "      \"unit\": \"hours\",\n",
            "      \"unit_price\": 50.00,\n",
            "      \"total\": 50.00\n",
            "    },\n",
            "    {\n",
            "      \"item_number\": 2,\n",
            "      \"description\": \"Develop\\n- Invoice number format template\\n- [PRO] Duplicating invoices\\n- Language support\\n- Context menu for invoices list\",\n",
            "      \"quantity\": 17.0,\n",
            "      \"unit\": \"hours\",\n",
            "      \"unit_price\": 40.00,\n",
            "      \"total\": 680.00\n",
            "    },\n",
            "    {\n",
            "      \"item_number\": 3,\n",
            "      \"description\": \"Analysis\\n- [PRO] Duplicating invoices\\n- Language support\",\n",
            "      \"quantity\": 3.0,\n",
            "      \"unit\": \"hours\",\n",
            "      \"unit_price\": 35.00,\n",
            "      \"total\": 105.00\n",
            "    }\n",
            "  ],\n",
            "  \"subtotal\": 835.00,\n",
            "  \"tax\": {\n",
            "    \"rate\": 18.0,\n",
            "    \"amount\": 150.30\n",
            "  },\n",
            "  \"discount\": {\n",
            "    \"rate\": 10.0,\n",
            "    \"amount\": -83.50\n",
            "  },\n",
            "  \"total\": 901.80,\n",
            "  \"currency\": \"EUR\"\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory for temporary uploaded files\n",
        "import os\n",
        "UPLOAD_DIR = \"/tmp/uploads\"\n",
        "\n",
        "# Ensure the upload directory exists\n",
        "os.makedirs(UPLOAD_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "QhUBbEessMZ1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio function that processes the image and user prompt\n",
        "def process_receipt(image,user_prompt):\n",
        "    # Define the path to save the uploaded image\n",
        "    image_path = os.path.join(UPLOAD_DIR, \"uploaded_receipt.png\")\n",
        "    image.save(image_path)\n",
        "    system_prompt = \"\"\"\n",
        "               You are a specialist in comprehending receipts.\n",
        "               Input images in the form of receipts will be provided to you,\n",
        "               and your task is to respond to questions based on the content of the input image.\n",
        "               \"\"\"\n",
        "\n",
        "    # Call the gemini_output function\n",
        "    output = gemini_output(image_path, system_prompt, user_prompt)\n",
        "\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "BdD9V316sWkB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "pip install gradio"
      ],
      "metadata": {
        "id": "rj0zCXWUsYst"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Gradio interface\n",
        "import gradio as gr\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"## Receipt Data Extraction\")\n",
        "    gr.Markdown(\"Upload a receipt image and provide a custom prompt for extracting information.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        image_input = gr.Image(label=\"Upload Receipt Image\", type=\"pil\")\n",
        "        user_prompt_input = gr.Textbox(label=\"User Prompt\", placeholder=\"E.g., 'Convert invoice data to JSON format'\")\n",
        "\n",
        "    output_display = gr.Textbox(label=\"Output\")\n",
        "\n",
        "    # Create a button to trigger the processing function\n",
        "    submit_button = gr.Button(\"Process Receipt\")\n",
        "\n",
        "    # Set up event handling\n",
        "    submit_button.click(fn=process_receipt,\n",
        "                        inputs=[image_input, user_prompt_input],\n",
        "                        outputs=output_display)\n",
        "\n",
        "# Launch the app\n",
        "app.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "xgPzxjgLshQC",
        "outputId": "85742370-8aab-46b5-e133-6e22ed94f999"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://4813793171774fa73a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4813793171774fa73a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://4813793171774fa73a.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/aparna181/Gen_ai-1.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVFzEhb-4Wsw",
        "outputId": "0adada14-3f87-4838-c832-3524e2dee61b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Gen_ai-1'...\n",
            "warning: You appear to have cloned an empty repository.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Gen_ai-1"
      ],
      "metadata": {
        "id": "SmUoCMVB4brO",
        "outputId": "8eda06ce-9b3c-4ec1-9591-e0a6c41a9b03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Gen_ai-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout -b feature-branch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_VyAwChsGDN",
        "outputId": "7fc62176-a3a6-462c-d390-6eb1703c2fcf"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Switched to a new branch 'feature-branch'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"# Gen_ai-1\" >> README.md\n"
      ],
      "metadata": {
        "id": "vbtCSlLQtwUn"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git init\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPHgSggKuhKw",
        "outputId": "1d39ecc8-64c8-4db5-8ef6-fa124967b090"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reinitialized existing Git repository in /content/Gen_ai-1/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add README.md\n"
      ],
      "metadata": {
        "id": "ixBZvRpHul6o"
      },
      "execution_count": 52,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmmlSGOmhp917CLxDUujfZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}